{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3871be37",
   "metadata": {},
   "source": [
    "### Model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad904cef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='<think>\\n\\n</think>\\n\\nHello! How can I assist you today? ðŸ˜Š', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 4, 'total_tokens': 20, 'completion_time': 0.079697988, 'prompt_time': 6.2188e-05, 'queue_time': 0.054578772000000005, 'total_time': 0.079760176}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_76307ac09b', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--bc93b25c-8372-449d-9ea3-aa44dfa0231d-0', usage_metadata={'input_tokens': 4, 'output_tokens': 16, 'total_tokens': 20})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "llm = ChatGroq(model=\"deepseek-r1-distill-llama-70b\")\n",
    "llm.invoke(\"hi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0d2d08",
   "metadata": {},
   "source": [
    "### Loading Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4757613",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "embedding_model = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\") \n",
    "# embedding_model.embed_query(\"hai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daba5e09",
   "metadata": {},
   "source": [
    "## 1. Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3690458",
   "metadata": {},
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d80146a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "file_path = os.path.join(\"..\", \"data\", \"sample.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdddf986",
   "metadata": {},
   "source": [
    "Document Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8b8ee1f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(file_path=file_path)\n",
    "document = loader.load()\n",
    "len(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea23687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ignore this, just skipping some pdf pages to make it easy.\n",
    "document = document[:20]\n",
    "len(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb74adfb",
   "metadata": {},
   "source": [
    "Document Spliiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "884e0d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=150,\n",
    "    length_function=len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9e814d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted_doc = splitter.split_documents(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf9e5e2",
   "metadata": {},
   "source": [
    "### Vector Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b55bef98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "vector_store = FAISS.from_documents(splitted_doc, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "96984dd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='3abaf823-0fca-4618-bfdb-2497e3f89dcb', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 7, 'page_label': '8'}, page_content='13B 18.9 66.1 52.6 62.3 10.9 46.9 37.0 33.9\\n33B 26.0 70.0 58.4 67.6 21.4 57.8 39.8 41.7\\n65B 30.7 70.7 60.5 68.6 30.8 63.4 43.5 47.6\\nLlama 2\\n7B 16.8 63.9 48.9 61.3 14.6 45.3 32.6 29.3\\n13B 24.5 66.9 55.4 65.8 28.7 54.8 39.4 39.1\\n34B 27.8 69.9 58.7 68.0 24.2 62.6 44.1 43.4\\n70B 37.5 71.9 63.6 69.4 35.2 68.9 51.2 54.2\\nTable 3: Overall performance on grouped academic benchmarks compared to open-source base models.'),\n",
       " Document(id='53c6674a-d11b-4995-acd0-cff0cade5d08', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 6, 'page_label': '7'}, page_content='models internally. For these models, we always pick the best score between our evaluation framework and\\nany publicly reported results.\\nIn Table 3, we summarize the overall performance across a suite of popular benchmarks. Note that safety\\nbenchmarks are shared in Section 4.1. The benchmarks are grouped into the categories listed below. The\\nresults for all the individual benchmarks are available in Section A.2.2.')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.similarity_search(\"llama2 finetuning benchmark experiments.\", k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74764dc4",
   "metadata": {},
   "source": [
    "## 2. Data Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe3e128",
   "metadata": {},
   "source": [
    "### Retrieval part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ac0281be",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ba869296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='3abaf823-0fca-4618-bfdb-2497e3f89dcb', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 7, 'page_label': '8'}, page_content='13B 18.9 66.1 52.6 62.3 10.9 46.9 37.0 33.9\\n33B 26.0 70.0 58.4 67.6 21.4 57.8 39.8 41.7\\n65B 30.7 70.7 60.5 68.6 30.8 63.4 43.5 47.6\\nLlama 2\\n7B 16.8 63.9 48.9 61.3 14.6 45.3 32.6 29.3\\n13B 24.5 66.9 55.4 65.8 28.7 54.8 39.4 39.1\\n34B 27.8 69.9 58.7 68.0 24.2 62.6 44.1 43.4\\n70B 37.5 71.9 63.6 69.4 35.2 68.9 51.2 54.2\\nTable 3: Overall performance on grouped academic benchmarks compared to open-source base models.'),\n",
       " Document(id='53c6674a-d11b-4995-acd0-cff0cade5d08', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 6, 'page_label': '7'}, page_content='models internally. For these models, we always pick the best score between our evaluation framework and\\nany publicly reported results.\\nIn Table 3, we summarize the overall performance across a suite of popular benchmarks. Note that safety\\nbenchmarks are shared in Section 4.1. The benchmarks are grouped into the categories listed below. The\\nresults for all the individual benchmarks are available in Section A.2.2.'),\n",
       " Document(id='966e8bfc-4108-4421-ab89-7767c110035c', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 1, 'page_label': '2'}, page_content='Contents\\n1 Introduction 3\\n2 Pretraining 5\\n2.1 Pretraining Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n2.2 Training Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n2.3 Llama 2Pretrained Model Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\\n3 Fine-tuning 8\\n3.1 Supervised Fine-Tuning (SFT) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9'),\n",
       " Document(id='5584f149-85c4-4e7c-b702-3cb79bd8fb9f', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 2, 'page_label': '3'}, page_content='be on par with some of the closed-source models, at least on the human evaluations we performed (see\\nFigures 1 and 3). We have taken measures to increase the safety of these models, using safety-specific data\\nannotation and tuning, as well as conducting red-teaming and employing iterative evaluations. Additionally,\\nthis paper contributes a thorough description of our fine-tuning methodology and approach to improving')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"llama2 finetuning benchmark experiments.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28817ef",
   "metadata": {},
   "source": [
    "## 3.Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c4f8fc",
   "metadata": {},
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ad7422c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='\\n        Answer the question based on the context provided below. \\n        If the context does not contain sufficient information, respond with: \\n        \"I do not have enough information about this.\"\\n\\n        Context: {context}\\n\\n        Question: {question}\\n\\n        Answer:')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "        Answer the question based on the context provided below. \n",
    "        If the context does not contain sufficient information, respond with: \n",
    "        \"I do not have enough information about this.\"\n",
    "\n",
    "        Context: {context}\n",
    "\n",
    "        Question: {question}\n",
    "\n",
    "        Answer:\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f806a2b1",
   "metadata": {},
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e5c49178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x0000018AEA8D7730>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000018AEA8D5180>, model_name='deepseek-r1-distill-llama-70b', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10aff7a9",
   "metadata": {},
   "source": [
    "parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ec120b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251dc479",
   "metadata": {},
   "source": [
    "Chaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "40d61c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "080af23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\" : retriever | format_docs ,\n",
    "        \"question\" : RunnablePassthrough() \n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9b438fb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<think>\\nOkay, I need to figure out how to answer the question about the Llama 2 fine-tuning benchmark experiments based on the provided context. Let me start by reading through the context carefully.\\n\\nFirst, I see that the context includes a table labeled Table 3, which summarizes overall performance on grouped academic benchmarks. The table lists different model sizes (like 7B, 13B, etc.) and their corresponding scores across various benchmarks. It mentions that for these models, the best score between their evaluation framework and publicly reported results is chosen. \\n\\nAdditionally, the context talks about hyperparameters used in training, such as the AdamW optimizer, learning rate schedule, weight decay, and gradient clipping. It also mentions that Figure 5(a) shows the training loss for Llama 2 with these hyperparameters. There's a section about pretraining where they started with an approach from Touvron et al. (2023), using an optimized auto-regressive transformer with improvements like more data cleaning, updated data mixes, more tokens, longer context lengths, and grouped-query attention.\\n\\nThe question is asking about the fine-tuning benchmark experiments for Llama 2. From the context, I know that Table 3 includes benchmark results, but it doesn't specifically distinguish between pretraining and fine-tuning. The details about hyperparameters and pretraining improvements are more about the training process rather than fine-tuning experiments.\\n\\nSince the context doesn't provide specific information about the fine-tuning process or experiments, such as datasets used for fine-tuning, tasks performed, or results from those experiments, I can't answer the question accurately. The information given is more about the overall model performance and pretraining details, not the fine-tuning aspects.\\n\\nTherefore, the appropriate response is to state that there's not enough information about the fine-tuning benchmark experiments.\\n</think>\\n\\nI do not have enough information about this.\""
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"tell  me about the llama2 finetuning benchmark experiments?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
